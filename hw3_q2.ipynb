{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1Y4ebFsvIzu571XNTW79k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardb1203/COMP590_NLP/blob/main/hw3_q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4FhdBS0Dpv4",
        "outputId": "68c7351e-4853-46e5-d705-9d8fc1ab972e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95656\n",
            "5020\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint, time\n",
        "# download the treebank corpus from nltk\n",
        "nltk.download ('treebank')\n",
        "# download the universal tagset from nltk\n",
        "nltk.download ('universal_tagset')\n",
        "# reading the Treebank tagged sentences\n",
        "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset = 'universal'))\n",
        "# split data into training and validation set in the ratio 80:20\n",
        "train_set , test_set = train_test_split ( nltk_data , train_size =0.95 , test_size =0.05 ,\n",
        "random_state =123)\n",
        "# create list of train and test tagged words\n",
        "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
        "# This is for you to test the correctness of your implementation when you test on the test set\n",
        "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
        "# create the list of words for testing without the tags . This is the list you should be testing on!\n",
        "test_words_without_tags = [ tup [0] for sent in test_set for tup in sent ]\n",
        "# number of tagged words in training\n",
        "print ( len( train_tagged_words ) )\n",
        "# number of words in testing\n",
        "print ( len( test_words_without_tags ) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b, calculating emission probabilities, pair is (word, tag)\n",
        "# p(w|t) = C(t,w) / C(t)\n",
        "def emission(pair):\n",
        "  # tag is stored in 2nd position \n",
        "  tag = pair[1]\n",
        "  # get tag count\n",
        "  Ctag = sum(1 for elt in train_tagged_words if elt[1] == tag)\n",
        "  # and joint count\n",
        "  Cpair = sum(1 for elt in train_tagged_words if elt == pair)\n",
        "  return Cpair / Ctag"
      ],
      "metadata": {
        "id": "z3XfznTVM4GL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all pos pairs for part c\n",
        "pos_pairs = [(train_tagged_words[i][1], train_tagged_words[i+1][1]) for i in range(len(train_tagged_words)-1)]"
      ],
      "metadata": {
        "id": "f3ZYx9v2T6rf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c, calculating emission probabilites given pair (tag1, tag2)\n",
        "# p (tag2 | tag1) = C(tag1, tag2) / C(tag1)\n",
        "def transition(pair):\n",
        "  # tag1, the previous tag\n",
        "  tag1 = pair[0]\n",
        "  # get previous tag count\n",
        "  Ctag1 = sum(1 for elt in pos_pairs if elt[0] == tag1)\n",
        "  # and joint count\n",
        "  Cpair = sum(1 for elt in pos_pairs if elt == pair)\n",
        "  return Cpair / Ctag1"
      ],
      "metadata": {
        "id": "xXjEIcpESQ1o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gives unique tags = 12\n",
        "# print(len(set(list(sum(pos_pairs, ())))))\n",
        "tags = set(list(sum(pos_pairs, ())))"
      ],
      "metadata": {
        "id": "d0WxcCa8cpUz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d, TxT matrix\n",
        "# construct the empty matrix\n",
        "transition_matrix = pd.DataFrame(columns=tags, index=tags)\n",
        "# fill in the probabilities by calling transition on each (row,col)\n",
        "for col, series in transition_matrix.iteritems():\n",
        "    for index, value in series.iteritems():\n",
        "      transition_matrix.at[index,col] = transition((index,col))\n"
      ],
      "metadata": {
        "id": "AUZeQzbRaw9n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transition_matrix.head()"
      ],
      "metadata": {
        "id": "VgTdE6DhetaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all transition probs\n",
        "def get_transitions():\n",
        "  transitions = {tag:None for tag in tags}\n",
        "\n",
        "  for col, series in transition_matrix.iteritems():\n",
        "      for index, value in series.iteritems():\n",
        "        # for every POS, create a dict of its transition probs\n",
        "        if transitions[index] == None:\n",
        "          transitions[index] = {}\n",
        "        # EX: NOUN: {\"X\": .0004, \"DET\": .05}\n",
        "        transitions[index][col] = transition((index, col))\n",
        "\n",
        "  return transitions"
      ],
      "metadata": {
        "id": "X6xOyG6PpHfn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # get the emission probabilities for the given observed words\n",
        "def get_emissions(observed):\n",
        "  emissions = {tag:None for tag in tags}\n",
        "  for tag in tags:\n",
        "    for word in observed:\n",
        "      if emissions[tag] == None:\n",
        "        emissions[tag] = {}\n",
        "      emissions[tag][word] = emission((word,tag))\n",
        "  return emissions\n"
      ],
      "metadata": {
        "id": "qMK9G8LuYK-n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing func\n",
        "# get_emissions(test_words_without_tags[0:5])\n",
        "# get_transitions()"
      ],
      "metadata": {
        "id": "PrBsWcfLY3Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smoothing(observed, emissions):\n",
        "   # handle unseen emissions as NOUN\n",
        "  for word in observed:\n",
        "    sum = 0\n",
        "    for pos in emissions:\n",
        "      sum += emissions[pos][word]\n",
        "    if sum == 0:\n",
        "      emissions[\"NOUN\"][word] = 1\n",
        "  return emissions"
      ],
      "metadata": {
        "id": "dbHbhLjerBG3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Viterbi Algorithm\n",
        "# Goal: Maximize the join probability of P(hidden states, observed states)\n",
        "\n",
        "# Input: A sentence without POS\n",
        "# Output: POS tags for sentence and probability\n",
        "# Required: Emission & Transition probabilities\n",
        "\n",
        "def viterbi(observed):\n",
        "  \n",
        "  # get emission probs for given observed states\n",
        "  emissions = get_emissions(observed)\n",
        "  # get transition probs for given POS tags\n",
        "  transitions = get_transitions()\n",
        "  # smooth\n",
        "  emissions = smoothing(observed, emissions)\n",
        "   # each dictionary in V stores the probabilities for all possible states at a certain time step \n",
        "   # with the keys being the POS tags and the values being the probabilities of reaching each tag at that time step.\n",
        "  V = [{}]\n",
        "  # keeps track of most probable path\n",
        "  path = {}\n",
        "\n",
        "  # Initialize base cases (t == 0)\n",
        "  for POS in tags:\n",
        "    # assume each x_1 follows a . tag\n",
        "    # multiply the transition prob from start token to each POS by the emission from each POS to start observed word\n",
        "      V[0][POS] = transitions[\".\"][POS] * (emissions[POS][observed[0]])\n",
        "      path[POS] = [POS]\n",
        "  \n",
        "  # Run Viterbi algorithm for t > 0\n",
        "  for t in range(1, len(observed)):\n",
        "      V.append({})\n",
        "      newpath = {}\n",
        "\n",
        "      for POS in tags:\n",
        "          # Find the maximum probability and corresponding previous POS\n",
        "          (prob, prev_POS) = max((V[t-1][prev_POS] * transitions[prev_POS][POS] * emissions[POS][observed[t]], prev_POS) \n",
        "                                  for prev_POS in tags)\n",
        "          V[t][POS]= prob\n",
        "          newpath[POS] = path[prev_POS] + [POS]\n",
        "\n",
        "      # Update the path with the new best path\n",
        "      path = newpath\n",
        "\n",
        "  # Find the maximum probability and corresponding path\n",
        "  (prob, POS) = max((V[len(observed)-1][POS], POS) for POS in tags)\n",
        "  return (prob, path[POS])"
      ],
      "metadata": {
        "id": "Riz0he1lc8kz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = test_words_without_tags[0:28]\n",
        "sent2 = test_words_without_tags[28:63]\n",
        "sent3 = test_words_without_tags[63:81]\n",
        "\n",
        "\n",
        "print(\"Sentence: \" + \" \".join(sent1) + \" Viterbi result: \" + str(viterbi(sent1)))\n",
        "print(\"Sentence: \" + \" \".join(sent2) + \" Viterbi result: \" + str(viterbi(sent2)))\n",
        "print(\"Sentence: \" + \" \".join(sent3) + \" Viterbi result: \" + str(viterbi(sent3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn-8HL-wfYE2",
        "outputId": "6f612957-b678-4798-cb03-733e3b7e4592"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Worksheets in a test-practice kit called * Learning Materials , sold * to schools across the country by Macmillan\\/McGraw-Hill School Publishing Co. , contain the same questions . Viterbi result: (7.671191658704445e-70, ['NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'VERB', 'X', 'NOUN', 'NOUN', '.', 'VERB', 'X', 'PRT', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', '.', 'VERB', 'DET', 'ADJ', 'NOUN', '.'])\n",
            "Sentence: Government officials , especially in Japan , probably would resist any onslaught of program trading by players trying * to shrug off the U.S. furor over their activities and marching abroad with their business . Viterbi result: (2.802551631041295e-94, ['NOUN', 'NOUN', '.', 'ADV', 'ADP', 'NOUN', '.', 'ADV', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'VERB', 'X', 'PRT', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'PRON', 'NOUN', 'CONJ', 'VERB', 'ADV', 'ADP', 'PRON', 'NOUN', '.'])\n",
            "Sentence: Ms. Kirkpatrick , the Journal 's deputy editorial features editor , worked in Tokyo for three years . Viterbi result: (3.708069254558624e-50, ['NOUN', 'NOUN', '.', 'DET', 'NOUN', 'PRT', 'ADJ', 'NOUN', 'NOUN', 'NOUN', '.', 'VERB', 'ADP', 'NOUN', 'ADP', 'NUM', 'NOUN', '.'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = viterbi(test_words_without_tags)[1]\n",
        "target = [pair[1] for pair in test_tagged_words]\n",
        "\n",
        "def accuracy(pred, target):\n",
        "  for i in range(len(pred)):\n",
        "    wrong = 0\n",
        "    if pred[i] != target[i]:\n",
        "      wrong += 1\n",
        "  correct = len(pred) - wrong\n",
        "  return correct / len(pred)\n",
        "\n",
        "print(accuracy(pred, target))"
      ],
      "metadata": {
        "id": "oS2yiEz53r-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}