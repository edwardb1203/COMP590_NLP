{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c15a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/edwardbaker/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/edwardbaker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.corpus import reuters\n",
    "import string\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e85ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(reuters.raw(file))\n",
    "\n",
    "# split into sentences for easier cleaning for bigram\n",
    "for i in range(len(text)):\n",
    "    text[i] = nltk.sent_tokenize(text[i])\n",
    "    for j in range(len(text[i])):\n",
    "        text[i][j] = text[i][j].replace(\"\\n\", \"\")\n",
    "# Now text is a list in which each element is a sentence collected from the Reuters news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c8092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Remove any URLs and numbers from each sentence\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        text[i][j] = re.sub(r'https?\\S+', '',   text[i][j])\n",
    "        text[i][j] = re.sub(r'\\d+', '',   text[i][j]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d20a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b Lowercase each sentence, tokenize it using the NLTK tokenizer.\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        text[i][j] = nltk.word_tokenize(text[i][j].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c736641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c Now remove all stopwords, punctuations in each documents\n",
    "# consider punctuations in string.punct-uation in python and stem all tokens.\n",
    "stop_words = stopwords.words('english')\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        text[i][j] = [word for word in text[i][j] if word not in stop_words and word not in string.punctuation]\n",
    "        for k in range(len(text[i][j])):\n",
    "            text[i][j][k] = nltk.PorterStemmer().stem(text[i][j][k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2904e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad with start and end tokens\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text[i])):\n",
    "        text[i][j].insert(0,\"start_token\")\n",
    "        text[i][j].append(\"end_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d585186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['start_token', 'china', 'daili', 'say', 'vermin', 'eat', 'pct', 'grain', 'stock', 'survey', 'provinc', 'seven', 'citi', 'show', 'vermin', 'consum', 'seven', 'pct', 'china', \"'s\", 'grain', 'stock', 'china', 'daili', 'said', 'end_token'], ['start_token', 'also', 'said', 'year', 'mln', 'tonn', 'pct', 'china', \"'s\", 'fruit', 'output', 'left', 'rot', 'mln', 'tonn', 'pct', 'veget', 'end_token'], ['start_token', 'paper', 'blame', 'wast', 'inadequ', 'storag', 'bad', 'preserv', 'method', 'end_token'], ['start_token', 'said', 'govern', 'launch', 'nation', 'programm', 'reduc', 'wast', 'call', 'improv', 'technolog', 'storag', 'preserv', 'greater', 'product', 'addit', 'end_token'], ['start_token', 'paper', 'gave', 'detail', 'end_token']]\n"
     ]
    }
   ],
   "source": [
    "print(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f5a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d creating the bigram model\n",
    "lookup = {}\n",
    "\n",
    "# go through every article\n",
    "for article in text:\n",
    "    # and every sentence in every article\n",
    "    for sent in article:\n",
    "        for i in range(len(sent)-1):\n",
    "            if sent[i] not in lookup:\n",
    "                lookup[sent[i]] = {}\n",
    "            try:\n",
    "                lookup[sent[i]][sent[i+1]] += 1\n",
    "            except:\n",
    "                lookup[sent[i]][sent[i+1]] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa35983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e convert counts to probabilities\n",
    "for tokens in lookup:\n",
    "    count =  sum(lookup[tokens].values())\n",
    "    for key in lookup[tokens]:\n",
    "        lookup[tokens][key] = lookup[tokens][key] / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24df13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f sentence generation\n",
    "import random\n",
    "def generate_sentence():\n",
    "    sentence = \"start_token\"\n",
    "    word = \"start_token\"\n",
    "    while word != \"end_token\":\n",
    "        # word is a 1 elt array containing the selected value from the prob distribution\n",
    "        word = random.choices(list(lookup[word].keys()), weights=lookup[word].values(), k=1)\n",
    "        word = word[0]\n",
    "        sentence = sentence + \" \" + word\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cbeba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: start_token sever compani also underwritten lt milex corp say exchang commiss nation seri six-month deposit import polici action countri agre mutual omaha bob dole also rate commerci bank portug tax incent bonus tonn sugar institut west texa refineri crude oil market lead '' end_token\n",
      "Sentence 2: start_token forecast current oil industri product stood semiconductor trade sanction japan end_token\n",
      "Sentence 3: start_token financ cost reduc event past eight pct said may record cotton govern help motiv end_token\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence 1: \" + generate_sentence())\n",
    "print(\"Sentence 2: \" + generate_sentence())\n",
    "print(\"Sentence 3: \" + generate_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42353e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
